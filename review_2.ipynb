{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzPXsfLyvoHw",
        "outputId": "33c66dbb-2434-4e05-9df1-4d15e6cf0f66"
      },
      "outputs": [],
      "source": [
        "#nltk natural language process toolkit, tokenization, stemming, and sentiment analysis.\n",
        "#SentimentIntensityAnalyser designed to analyze sentiment in text ,\n",
        "#Valence Aware Dictionary and Sentiment Reasoner (VADER) particularly effective on social media tweets\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "analyzer=SentimentIntensityAnalyzer()\n",
        "def sentiment_analysis(tweet):\n",
        "    score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqq3JUH43IL7"
      },
      "source": [
        "Importing our data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "OtMiNa1Dvp_s",
        "outputId": "de5151f9-4fc3-45ce-8663-66a74c96cd13"
      },
      "outputs": [],
      "source": [
        "tweet_data=pd.read_csv('C:\\Users\\Dhanush kumar\\Downloads\\archive (3)\\stock_tweets.csv')\n",
        "tweet_data.rename(columns={\"Stock Name\": \"Stock_Name\"}, inplace=True)\n",
        "tweet_data.rename(columns={\"Company Name\": \"Company_Name\"}, inplace=True)\n",
        "tweet_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDZE7u6yvv8v"
      },
      "outputs": [],
      "source": [
        "stock_data=pd.read_csv(\"C:\\Users\\Dhanush kumar\\Downloads\\archive (3)\\stock_yfinance_data.csv\")\n",
        "stock_data.rename(columns={\"Stock Name\": \"Stock_Name\"}, inplace=True)\n",
        "stock_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFnYCKAsxkIm"
      },
      "outputs": [],
      "source": [
        "stock_data[\"Date\"] = pd.to_datetime(stock_data[\"Date\"]).dt.strftime(\"%Y-%m-%d\")\n",
        "stock_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAuxR9MvtjS6"
      },
      "outputs": [],
      "source": [
        "# Function to fetch historical data\n",
        "def fetch_historical_data(symbol):\n",
        "    data = yf.download(symbol, start=\"2020-01-01\", end=\"2024-10-17\")\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xita-T9xm8X"
      },
      "outputs": [],
      "source": [
        "##  add column to the stock price dataframe which shows the max stock price fluctuation\n",
        "# flucation price difference between high and low\n",
        "stock_data['Fluctuation'] = stock_data.High - stock_data.Low\n",
        "## add column to the stock price dataframe which shows the net rise in stock price\n",
        "#price_Gain difference between open and close price\n",
        "stock_data['Price_Gain'] = stock_data.Close - stock_data.Open\n",
        "##  add column to the stock price dataframe which shows the total valuation at the end of the day\n",
        "# total_Valuation_EOD(end of day)\n",
        "stock_data['Total_Valuation_EOD'] = stock_data.Volume * stock_data.Close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlcDfWoGxr3e"
      },
      "outputs": [],
      "source": [
        "stock_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arDiur7YsY_H"
      },
      "source": [
        "Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTBT_IkOxtTv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "#re used to import rregular expressions\n",
        "# Convert 'Tweet' column to string type to handle potential NaNs or numbers\n",
        "tweet_data['Tweet'] = tweet_data['Tweet'].astype(str)\n",
        "tweet_data['Tweet'] = tweet_data.apply(lambda row: row['Tweet'].lower(),axis=1) #removed capitalisation\n",
        "tweet_data['Tweet'] = tweet_data.apply(lambda row: re.sub(\"@[A-Za-z0-9_]+\",\"\", row['Tweet']),axis=1) #removed mentions\n",
        "tweet_data['Tweet'] = tweet_data.apply(lambda row: re.sub(\"#[A-Za-z0-9_]+\",\"\", row['Tweet']),axis=1) #removed hashtags\n",
        "tweet_data['Tweet'] = tweet_data.apply(lambda row: re.sub(r\"http\\S+\",\"\", row['Tweet']),axis=1) #removed websites\n",
        "tweet_data['Tweet'] = tweet_data.apply(lambda row: re.sub(r\"www.\\S+\",\"\", row['Tweet']),axis=1)\n",
        "tweet_data['Tweet'] = tweet_data.apply(lambda row: re.sub('[()!?]',\" \", row['Tweet']),axis=1) #removed puncs\n",
        "tweet_data['Tweet'] = tweet_data.apply(lambda row: re.sub('\\[.*?\\]',\" \", row['Tweet']),axis=1)\n",
        "tweet_data['Tweet'] = tweet_data.apply(lambda row: re.sub(\"[^a-z]\",\" \", row['Tweet']),axis=1)\n",
        "\n",
        "tweet_data[['Tweet']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63Jn6Be6x-Ek"
      },
      "outputs": [],
      "source": [
        "tweet_data['Sentiment'] = tweet_data['Tweet'].apply(lambda x : sentiment_analysis(x))\n",
        "tweet_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ5kp90x2PpH"
      },
      "outputs": [],
      "source": [
        "#creting date_string for tweet_data\n",
        "#anchor is created by adding tweet date and stock name\n",
        "# creating Anchor Column for tweet_data\n",
        "tweet_data.insert(1, \"Date_string\", tweet_data.Date.astype(\"str\").str.split(\" \"))\n",
        "tweet_data.Date_string = [element[0] for element in tweet_data.Date_string]\n",
        "tweet_data.insert(0, \"anchor\", tweet_data.Date_string + tweet_data.Stock_Name)\n",
        "\n",
        "#creating date_string for stock_data\n",
        "# creating Anchor Column for stock_data\n",
        "stock_data.insert(1, \"Date_string\", stock_data.Date.astype(\"str\").str.split(\" \"))\n",
        "stock_data.Date_string = [element[0] for element in stock_data.Date_string]\n",
        "stock_data.insert(0, \"anchor\", stock_data.Date_string + stock_data.Stock_Name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHGGG-Wa3VYp"
      },
      "outputs": [],
      "source": [
        "tweet_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK2Muz-E7yL_"
      },
      "source": [
        "Show Distribution of Positive , negative and netural counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ57U1WU3Z8l"
      },
      "outputs": [],
      "source": [
        "positive_count = (tweet_data['Sentiment'] > 0).sum()  # count positive values\n",
        "negative_count = (tweet_data['Sentiment'] < 0).sum()  # count negative values\n",
        "zero_count = (tweet_data['Sentiment'] == 0).sum()  # count zero values\n",
        "\n",
        "# display counts\n",
        "print(\"Positive Count:\", positive_count)\n",
        "print(\"Negative Count:\", negative_count)\n",
        "print(\"Zero Count:\", zero_count)\n",
        "\n",
        "labels = ['Positive', 'Negative' , 'Zero']\n",
        "sizes = [positive_count, negative_count, zero_count]\n",
        "colors = ['g', 'r', 'y' ]\n",
        "\n",
        "# pie chart\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "plt.axis('equal')\n",
        "plt.title('Distribution of Positive and Negative')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu84Rjmy8CFW"
      },
      "source": [
        "The above pie chart shows the distribution of positive, negative and netural values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7shWDa5s3c0O"
      },
      "outputs": [],
      "source": [
        "stock_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBUHVL0a3l0P"
      },
      "outputs": [],
      "source": [
        "# merging the two dataframe on the anchor in single data frame\n",
        "\n",
        "data = pd.merge(tweet_data, stock_data , on=\"anchor\")\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFLbEPm23sh6"
      },
      "outputs": [],
      "source": [
        "# let us convert the string date column \"date_str_x\" to datetime\n",
        "data.Date_string_x = pd.to_datetime(data.Date_string_x)\n",
        "# since we are aiming to see the impact of tweets on stock value (i.e. rise and fall),\n",
        "# we can drop \"neutral\" sentiments\n",
        "data = data[data.Sentiment != 0]\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgmWpCJ93yV7"
      },
      "outputs": [],
      "source": [
        "## only keeping the valuable  necessary data from processing\n",
        "preprocessed_data=data[\n",
        "    [\n",
        "        \"Date_x\",\n",
        "        \"Date_string_x\",\n",
        "        \"Tweet\",\n",
        "        \"Stock_Name_x\",\n",
        "        \"Company_Name\",\n",
        "        \"Sentiment\",\n",
        "        \"Open\",\n",
        "        \"High\",\n",
        "        \"Low\",\n",
        "        \"Close\",\n",
        "        \"Volume\",\n",
        "        \"Fluctuation\",\n",
        "        \"Price_Gain\",\n",
        "        \"Total_Valuation_EOD\"\n",
        "    ]\n",
        "]\n",
        "preprocessed_data = preprocessed_data.copy()\n",
        "## counter is use to count number of positive and negative tweets per day\n",
        "## counter= daily tweet volume\n",
        "preprocessed_data[\"counter\"] = 1\n",
        "preprocessed_data.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZqbbGDZ-KUL"
      },
      "source": [
        "Processing no of psitive trend per-day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S76J3stA3zNJ"
      },
      "outputs": [],
      "source": [
        "positive_tweets = preprocessed_data[preprocessed_data['Sentiment'] > 0]\n",
        "positive_tweets_per_day = positive_tweets.groupby('Date_string_x').size()\n",
        "plt.figure(figsize=(15, 6))\n",
        "positive_tweets_per_day.plot(kind='line', marker='o', color='green')\n",
        "plt.title('Number of Positive Tweets per Day')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Positive Tweets')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5uU-bJr-tgn"
      },
      "source": [
        "charts of total no of negative trends tweets per-day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfd9FnyK4gOW"
      },
      "outputs": [],
      "source": [
        "negative_tweets = preprocessed_data[preprocessed_data['Sentiment'] < 0]\n",
        "negative_tweets_per_day = negative_tweets.groupby('Date_string_x').size()\n",
        "plt.figure(figsize=(15, 6))\n",
        "negative_tweets_per_day.plot(kind='line', marker='o', color='red')\n",
        "plt.title('Number of Negative Tweets per Day')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Negative Tweets')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyAQdi7d_EBM"
      },
      "source": [
        "Showing total comapanies and no of tweets by company"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E25sa-jq4jpW"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"In our dataset, we have total {len(preprocessed_data.Company_Name.value_counts())} companies, namely\\n{preprocessed_data.Company_Name.value_counts()}\"\n",
        ")\n",
        "## pie chart of number of tweets to company\n",
        "\n",
        "company_counts = tweet_data['Company_Name'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(company_counts, labels=company_counts.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Number of Tweets by Company')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8gVCIX-_P0K"
      },
      "source": [
        "Making individual  dataset fo r each companies for  APPLE (APPL), TESLA (TSLA) ,TAIWAN SEMICONDUCTOR (TSM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBqVwQYQ4mYo"
      },
      "outputs": [],
      "source": [
        "# let us make datasets for top 3 companies\n",
        "tesla_df = preprocessed_data[preprocessed_data.Stock_Name_x == \"TSLA\"]\n",
        "taiwanSMC_df = preprocessed_data[preprocessed_data.Stock_Name_x == \"TSM\"]\n",
        "apple_df = preprocessed_data[preprocessed_data.Stock_Name_x == \"AAPL\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NHNNvYsAAaC"
      },
      "source": [
        "Creating dataframe of company based on positive and negative tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX0s-Tac4q1f"
      },
      "outputs": [],
      "source": [
        "# for simplicity, we will further form 2 sub dataframes per company based on the sentiments: positive and negative\n",
        "pos_tesla_df = tesla_df[tesla_df.Sentiment > 0]\n",
        "pos_taiwanSMC_df = taiwanSMC_df[taiwanSMC_df.Sentiment >0]\n",
        "pos_apple_df = apple_df[apple_df.Sentiment > 0]\n",
        "\n",
        "neg_tesla_df = tesla_df[tesla_df.Sentiment <0]\n",
        "neg_taiwanSMC_df = taiwanSMC_df[taiwanSMC_df.Sentiment <0]\n",
        "neg_apple_df = apple_df[apple_df.Sentiment <0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUeNkJ1D41gh"
      },
      "outputs": [],
      "source": [
        "# let us create dataset with limited values that give us a brief info about rise and fall in total valuation of the company over time\n",
        "ovr_pos_tesla_df = pos_tesla_df.groupby(by=[\"Date_string_x\",\"Fluctuation\", \"Price_Gain\", \"Total_Valuation_EOD\",\"Sentiment\"], as_index=False).agg({\"counter\":pd.Series.sum})\n",
        "ovr_pos_taiwanSMC_df = pos_taiwanSMC_df.groupby(by=[\"Date_string_x\",\"Fluctuation\", \"Price_Gain\", \"Total_Valuation_EOD\",\"Sentiment\"], as_index=False).agg({\"counter\":pd.Series.sum})\n",
        "ovr_pos_apple_df = pos_apple_df.groupby(by=[\"Date_string_x\",\"Fluctuation\", \"Price_Gain\", \"Total_Valuation_EOD\",\"Sentiment\"], as_index=False).agg({\"counter\":pd.Series.sum})\n",
        "\n",
        "ovr_neg_tesla_df = neg_tesla_df.groupby(by=[\"Date_string_x\",\"Fluctuation\", \"Price_Gain\", \"Total_Valuation_EOD\",\"Sentiment\"], as_index=False).agg({\"counter\":pd.Series.sum})\n",
        "ovr_neg_taiwanSMC_df = neg_taiwanSMC_df.groupby(by=[\"Date_string_x\",\"Fluctuation\", \"Price_Gain\", \"Total_Valuation_EOD\",\"Sentiment\"], as_index=False).agg({\"counter\":pd.Series.sum})\n",
        "ovr_neg_apple_df = neg_apple_df.groupby(by=[\"Date_string_x\",\"Fluctuation\", \"Price_Gain\", \"Total_Valuation_EOD\",\"Sentiment\"], as_index=False).agg({\"counter\":pd.Series.sum})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5nuDdSSAPBS"
      },
      "source": [
        "CASE 1 : TESLA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VyXdI_Z45_I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.title(\"Tesla: Effect of positive Tweets on Valuation\")\n",
        "ax1 = plt.gca()\n",
        "ax2 = plt.twinx()\n",
        "\n",
        "\n",
        "ax1.plot(\n",
        "    pos_tesla_df.Date_string_x,\n",
        "    pos_tesla_df.Total_Valuation_EOD,\n",
        "    color=\"y\",\n",
        "    label=\"Valuation\",\n",
        ")\n",
        "\n",
        "\n",
        "positive_tweets = tesla_df[tesla_df['Sentiment'] > 0]\n",
        "positive_tweets_per_day = positive_tweets.groupby('Date_string_x').size()\n",
        "\n",
        "\n",
        "ax2.plot(\n",
        "    pos_tesla_df.Date_string_x.unique(),\n",
        "    positive_tweets_per_day,\n",
        "    color=\"g\",\n",
        "    label=\"Positive Tweets\",\n",
        ")\n",
        "\n",
        "ax1.set_xlabel(\"Time\")\n",
        "ax1.set_ylabel(\"Valuation\")\n",
        "ax2.set_ylabel(\"Positive Tweets\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmsAnYmu46hr"
      },
      "outputs": [],
      "source": [
        "# correlation matrix\n",
        "corr = ovr_pos_tesla_df.corr()\n",
        "\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "# drawing heatmap\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    mask=mask,\n",
        "    cmap=cmap,\n",
        "    annot=True,\n",
        "    vmax=.3,\n",
        "    vmin=-.3,\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=.5,\n",
        "    cbar_kws={\"shrink\": .5}\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c85Vhyqa49oE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "plt.title(\"Tesla: Effect of negative Tweets on Valuation\")\n",
        "ax1 = plt.gca()\n",
        "ax2 = plt.twinx()\n",
        "\n",
        "\n",
        "ax1.plot(\n",
        "    neg_tesla_df.Date_string_x,\n",
        "    np.log(neg_tesla_df.Price_Gain),\n",
        "    color=\"y\",\n",
        "    label=\"Valuation\",\n",
        ")\n",
        "\n",
        "\n",
        "negative_tweets = tesla_df[tesla_df['Sentiment'] < 0]\n",
        "negative_tweets_per_day = negative_tweets.groupby('Date_string_x').size()\n",
        "\n",
        "\n",
        "ax2.plot(\n",
        "    neg_tesla_df.Date_string_x.unique(),\n",
        "    negative_tweets_per_day,\n",
        "    color=\"r\",\n",
        "    label=\"Negative Tweets\",\n",
        ")\n",
        "\n",
        "ax1.set_xlabel(\"Time\")\n",
        "ax1.set_ylabel(\"Valuation\")\n",
        "ax2.set_ylabel(\"Negative Tweets\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiehEtlw5YiF"
      },
      "outputs": [],
      "source": [
        "# correlation matrix\n",
        "corr = ovr_neg_tesla_df.corr()\n",
        "\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "# drawing heatmap\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    mask=mask,\n",
        "    cmap=cmap,\n",
        "    annot=True,\n",
        "    vmax=.3,\n",
        "    vmin=-.3,\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=.5,\n",
        "    cbar_kws={\"shrink\": .5}\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0VDJDOgAj17"
      },
      "source": [
        "CASE 2 : TAIWAN SMC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAGVvPnP5crp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "plt.title(\"Taiwan SMC: Effect of Positive Tweets on Valuation\")\n",
        "ax1 = plt.gca()\n",
        "ax2 = plt.twinx()\n",
        "\n",
        "\n",
        "ax1.plot(\n",
        "    pos_taiwanSMC_df.Date_string_x,\n",
        "    pos_taiwanSMC_df.Total_Valuation_EOD,\n",
        "    color=\"y\",\n",
        "    label=\"Valuation\",\n",
        ")\n",
        "\n",
        "\n",
        "positive_tweets = taiwanSMC_df[taiwanSMC_df['Sentiment'] >0]\n",
        "positive_tweets_per_day = positive_tweets.groupby('Date_string_x').size()\n",
        "\n",
        "\n",
        "ax2.plot(\n",
        "    pos_taiwanSMC_df.Date_string_x.unique(),\n",
        "    positive_tweets_per_day,\n",
        "    color=\"g\",\n",
        "    label=\"Positive Tweets\",\n",
        ")\n",
        "\n",
        "ax1.set_xlabel(\"Time\")\n",
        "ax1.set_ylabel(\"Valuation\")\n",
        "ax2.set_ylabel(\"Positive Tweets\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "333Dcep75rBJ"
      },
      "outputs": [],
      "source": [
        "# correlation matrix\n",
        "corr = ovr_pos_taiwanSMC_df.corr()\n",
        "\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "# drawing heatmap\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    mask=mask,\n",
        "    cmap=cmap,\n",
        "    annot=True,\n",
        "    vmax=.3,\n",
        "    vmin=-.3,\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=.5,\n",
        "    cbar_kws={\"shrink\": .5}\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BGWCUHd5vyN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "plt.title(\"Taiwan SMC: Effect of negative Tweets on Valuation\")\n",
        "ax1 = plt.gca()\n",
        "ax2 = plt.twinx()\n",
        "\n",
        "\n",
        "ax1.plot(\n",
        "    neg_taiwanSMC_df.Date_string_x,\n",
        "    np.log(neg_taiwanSMC_df.Price_Gain),\n",
        "    color=\"y\",\n",
        "    label=\"Valuation\",\n",
        ")\n",
        "\n",
        "\n",
        "negative_tweets = taiwanSMC_df[taiwanSMC_df['Sentiment'] <0]\n",
        "negative_tweets_per_day = negative_tweets.groupby('Date_string_x').size()\n",
        "\n",
        "\n",
        "ax2.plot(\n",
        "    neg_taiwanSMC_df.Date_string_x.unique(),\n",
        "    negative_tweets_per_day,\n",
        "    color=\"r\",\n",
        "    label=\"Negative Tweets\",\n",
        ")\n",
        "\n",
        "ax1.set_xlabel(\"Time\")\n",
        "ax1.set_ylabel(\"Valuation\")\n",
        "ax2.set_ylabel(\"Negative Tweets\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ7O-p2A5y-Q"
      },
      "outputs": [],
      "source": [
        "# correlation matrix\n",
        "corr = ovr_neg_taiwanSMC_df.corr()\n",
        "\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "# drawing heatmap\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    mask=mask,\n",
        "    cmap=cmap,\n",
        "    annot=True,\n",
        "    vmax=.3,\n",
        "    vmin=-.3,\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=.5,\n",
        "    cbar_kws={\"shrink\": .5}\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxla71kDA0FY"
      },
      "source": [
        "CASE 3 : APPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPxF5sJf51aI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "plt.title(\"Apple: Effect of Positive Tweets on Valuation\")\n",
        "ax1 = plt.gca()\n",
        "ax2 = plt.twinx()\n",
        "\n",
        "\n",
        "ax1.plot(\n",
        "    pos_apple_df.Date_string_x,\n",
        "    pos_apple_df.Total_Valuation_EOD,\n",
        "    color=\"y\",\n",
        "    label=\"Valuation\",\n",
        ")\n",
        "\n",
        "positive_tweets = apple_df[apple_df['Sentiment'] > 0]\n",
        "positive_tweets_per_day = positive_tweets.groupby('Date_string_x').size()\n",
        "\n",
        "\n",
        "ax2.plot(\n",
        "    pos_apple_df.Date_string_x.unique(),\n",
        "    positive_tweets_per_day,\n",
        "    color=\"g\",\n",
        "    label=\"Positive Tweets\",\n",
        ")\n",
        "\n",
        "ax1.set_xlabel(\"Time\")\n",
        "ax1.set_ylabel(\"Valuation\")\n",
        "ax2.set_ylabel(\"Positive Tweets\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDXeU30-BAHt"
      },
      "source": [
        "creating heat for apple company dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kVas_X056T0"
      },
      "outputs": [],
      "source": [
        "# correlation matrix\n",
        "corr = ovr_pos_apple_df.corr()\n",
        "\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "# drawing heatmap\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    mask=mask,\n",
        "    cmap=cmap,\n",
        "    annot=True,\n",
        "    vmax=.3,\n",
        "    vmin=-.3,\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=.5,\n",
        "    cbar_kws={\"shrink\": .5}\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HUaRSWi5-NE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "plt.title(\"Apple: Effect of negative Tweets on Valuation\")\n",
        "ax1 = plt.gca()\n",
        "ax2 = plt.twinx()\n",
        "\n",
        "\n",
        "ax1.plot(\n",
        "    neg_apple_df.Date_string_x,\n",
        "    np.log(neg_apple_df.Price_Gain),\n",
        "    color=\"y\",\n",
        "    label=\"Valuation\",\n",
        ")\n",
        "\n",
        "\n",
        "negative_tweets = apple_df[apple_df['Sentiment'] < 0]\n",
        "negative_tweets_per_day = negative_tweets.groupby('Date_string_x').size()\n",
        "\n",
        "\n",
        "ax2.plot(\n",
        "    neg_apple_df.Date_string_x.unique(),\n",
        "    negative_tweets_per_day,\n",
        "    color=\"r\",\n",
        "    label=\"Negative Tweets\",\n",
        ")\n",
        "\n",
        "ax1.set_xlabel(\"Time\")\n",
        "ax1.set_ylabel(\"Valuation\")\n",
        "ax2.set_ylabel(\"Negative Tweets\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvCFCJQP6FGN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# correlation matrix\n",
        "corr = ovr_neg_apple_df.corr()\n",
        "\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# drawing heatmap\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    mask=mask,\n",
        "    cmap=cmap,\n",
        "    annot=True,\n",
        "    vmax=.3,\n",
        "    vmin=-.3,\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=.5,\n",
        "    cbar_kws={\"shrink\": .5}\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtgAmtFMzLvp"
      },
      "source": [
        "Tesla Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4wWvtnoBRHg"
      },
      "source": [
        "SVM based prediction of stock movement based on sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8uy7opS6Jbl"
      },
      "outputs": [],
      "source": [
        "## importing libraries\n",
        "#using SVM support vector machine algorithm\n",
        "#to find a hyperplane that, to the best degree possible,\n",
        "# separates data points of one class from those of another class\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "# train_test_split creating training and test data sets.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#MinMaxscalerStandardizes features by scaling each feature to a given range\n",
        "#taking necessary columns from the dataframe\n",
        "tesla=tesla_df\n",
        "drop=['Date_x','Stock_Name_x','Company_Name','Sentiment','Open','High','Low','Volume','Fluctuation','Price_Gain','Total_Valuation_EOD']\n",
        "tesla.drop(columns=drop, inplace=True)\n",
        "tesla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG5wchU0Eu33"
      },
      "source": [
        "getting sentiment score for the tweet based on positive , negative and netural"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shlqIjj36MIV"
      },
      "outputs": [],
      "source": [
        "def get_sentiment_scores(tweet):\n",
        "    scores = analyzer.polarity_scores(tweet)\n",
        "    return scores\n",
        "\n",
        "tesla['Sentiment_Scores'] = tesla['Tweet'].apply(get_sentiment_scores)\n",
        "tesla[['Positive', 'Neutral', 'Negative', 'Compound']] = tesla['Sentiment_Scores'].apply(pd.Series)\n",
        "tesla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln5vY6_P6QW4"
      },
      "outputs": [],
      "source": [
        "tesla['Sentiment']=tesla['Compound']\n",
        "drop=['Sentiment_Scores','Positive','Neutral','Negative','Compound','Tweet']\n",
        "tesla.drop(columns=drop, inplace=True)\n",
        "tesla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb1KctY_Qj5G"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuByu4yb6YCt"
      },
      "outputs": [],
      "source": [
        "tesla2=tesla\n",
        "\n",
        "tesla2['Date'] = pd.to_datetime(tesla2['Date_string_x']).dt.date\n",
        "\n",
        "# grouping by date, calculate mean sentiment and count number of tweets\n",
        "final = tesla2.groupby('Date').agg({\n",
        "    'Sentiment': 'mean',\n",
        "    'Date_string_x': 'count',\n",
        "    'Close': 'last'\n",
        "}).reset_index()\n",
        "final.columns = ['Date', 'Mean_Sentiment', 'Num_Tweets', 'Close']\n",
        "print(final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olb5ZVVHFn5J"
      },
      "source": [
        "predicting value   based on 3 adys of previous closing price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8sBEtbfGRnx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HwNgHGZ6gaU"
      },
      "outputs": [],
      "source": [
        "## predicting closing value of stock based on a 3 day window of previous closing prices, sentiments, and number of tweets\n",
        "#sliding window, Feature Engineering, Time Series Forecasting\n",
        "def window_data(df, window, feature_col_number1, feature_col_number2, feature_col_number3, target_col_number):\n",
        "    # creating empty lists \"X_close\", \"X_sentiment\", \"X_ts\" and y\n",
        "    X_close = []\n",
        "    X_sentiment = []\n",
        "    X_ts = []\n",
        "    y = []\n",
        "    for i in range(len(df) - window):\n",
        "\n",
        "        close = df.iloc[i:(i + window), feature_col_number1]\n",
        "        ts_sentiment = df.iloc[i:(i + window), feature_col_number2]\n",
        "        tw_vol = df.iloc[i:(i + window), feature_col_number3]\n",
        "        target = df.iloc[(i + window), target_col_number]\n",
        "\n",
        "        X_close.append(close)\n",
        "        X_sentiment.append(ts_sentiment)\n",
        "        X_ts.append(tw_vol)\n",
        "        y.append(target)\n",
        "\n",
        "    return np.hstack((X_close,X_sentiment,X_ts)), np.array(y).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh_b3RDc6lxG"
      },
      "outputs": [],
      "source": [
        "#window_size  function will use the past 3 days of data to predict the closing price of a stock.\n",
        "window_size = 3\n",
        "# column index 3 is the `Close` column\n",
        "# column index 1 is the `Mean_Sentiment` column\n",
        "# column index 2 is the `Num_tweets` column\n",
        "feature_col_number1 = 3\n",
        "feature_col_number2 = 1\n",
        "feature_col_number3 = 2\n",
        "target_col_number = 3\n",
        "X, y = window_data(final, window_size, feature_col_number1, feature_col_number2, feature_col_number3, target_col_number)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxHJAXyz6mJw"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=21)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8fcA88s6o2R"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "model = svm.SVR()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train.ravel())\n",
        "print(model.score(X_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eU7G5Jj6rsA"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)\n",
        "print(r2_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2NTnD6h6uJt"
      },
      "outputs": [],
      "source": [
        "predicted_prices = y_pred.reshape(-1, 1)\n",
        "real_prices = y_test.reshape(-1, 1)\n",
        "stocks = pd.DataFrame({\n",
        "    \"Real\": real_prices.ravel(),\n",
        "    \"Predicted\": predicted_prices.ravel()\n",
        "})\n",
        "stocks['Date'] = final['Date'][-len(real_prices):].reset_index(drop=True)\n",
        "\n",
        "stocks.set_index('Date', inplace=True)\n",
        "stocks.head()\n",
        "\n",
        "plt.plot(stocks['Real'], color='r', label='real')\n",
        "plt.plot(stocks['Predicted'], color='g', label='predicted')\n",
        "plt.title(\"Actual vs Predicted Values: SVM\")\n",
        "plt.xlabel('Date')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MME8SqbPHf1H"
      },
      "source": [
        "Observation :\n",
        "Clearly we can see that the tweet volume and sentiment score are unable to predict the closing price of stock.\n",
        "But can we infer the general trend of the market using the tweet volume and sentiment score. This can be done by comparing slopes of the predicted price and real price of stock with time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhiNGTMe6wJp"
      },
      "outputs": [],
      "source": [
        "\n",
        "real_values = stocks['Real'].values\n",
        "predicted_values = stocks['Predicted'].values\n",
        "\n",
        "real_slopes = np.diff(real_values)\n",
        "real_slopes = np.insert(real_slopes, 0, 0)\n",
        "\n",
        "predicted_slopes = np.diff(predicted_values)\n",
        "predicted_slopes = np.insert(predicted_slopes, 0, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndOVOHrD6254"
      },
      "outputs": [],
      "source": [
        "real_slope_signs = np.sign(real_slopes)\n",
        "\n",
        "predicted_slope_signs = np.sign(predicted_slopes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOjorlzQHnI_"
      },
      "source": [
        "Accuracy of comparing slopes is 80.95% showing that while tweet volume and sentiment score are not good metrics to model the closing price of a stock. They can be used to predict the general trend of the market to a good degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZMQ-7X16443"
      },
      "outputs": [],
      "source": [
        "matching_signs = (real_slope_signs == predicted_slope_signs)\n",
        "\n",
        "accuracy = np.mean(matching_signs) * 100\n",
        "print(f\"Accuracy of slope sign comparison: {accuracy}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhCq_Rrdy1hQ"
      },
      "source": [
        "Linear Reg\n",
        "This is a class from the sklearn.linear_model module used to create a linear regression model.\n",
        "it attempts to model the relationship between a dependent variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzUDAiPQ6-U5"
      },
      "outputs": [],
      "source": [
        "tesla = tweet_data[tweet_data['Stock_Name'] == 'TSLA']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y414Rnpx7UXh"
      },
      "outputs": [],
      "source": [
        "tesla = tesla[tesla['Sentiment'] != 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69T8B0VU7VvN"
      },
      "outputs": [],
      "source": [
        "tesla = tesla.drop(columns=['anchor', 'Date', 'Company_Name'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpzECeVO7XHd"
      },
      "outputs": [],
      "source": [
        "tesla.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKSlPnYf7Yax"
      },
      "outputs": [],
      "source": [
        "dates = tesla['Date_string'].unique()\n",
        "average = {}\n",
        "for i in dates:\n",
        "    filtered_date = tesla[tesla['Date_string'] == i]\n",
        "    average_value = filtered_date['Sentiment'].mean()\n",
        "    average[i] = average_value\n",
        "tesla_f = pd.DataFrame(list(average.items()), columns=['Date', 'Sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIv8tEvb7bGE"
      },
      "outputs": [],
      "source": [
        "tesla_f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsdiB4zo7cs5"
      },
      "outputs": [],
      "source": [
        "tesla_stock = stock_data[stock_data['Stock_Name'] == 'TSLA']\n",
        "stock_f = tesla_stock.drop(columns=['anchor', 'Date_string', 'Open', 'High', 'Low', 'Adj Close', 'Volume', 'Stock_Name', 'Fluctuation', 'Price_Gain', 'Total_Valuation_EOD'], axis=1)\n",
        "stock_f.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0SLq-317fJh"
      },
      "outputs": [],
      "source": [
        "tesla_p = pd.merge(tesla_f, stock_f, on='Date', how='inner')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-WvO58_8a61"
      },
      "outputs": [],
      "source": [
        "\n",
        "tesla_p['Close_1'] = tesla_p['Close']\n",
        "tesla_p['Close_2'] = tesla_p['Close']\n",
        "tesla_p['Close_3'] = tesla_p['Close']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tw_F1I_8cnZ"
      },
      "outputs": [],
      "source": [
        "tesla_p['Close_1'] = tesla_p['Close'].shift(1)\n",
        "tesla_p['Close_2'] = tesla_p['Close'].shift(2)\n",
        "tesla_p['Close_3'] = tesla_p['Close'].shift(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBrF2Oeu8edh"
      },
      "outputs": [],
      "source": [
        "tesla_p = tesla_p.drop(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UudVzWs48fy8"
      },
      "outputs": [],
      "source": [
        "tesla_p = tesla_p.drop(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s5qiljh8jrz"
      },
      "outputs": [],
      "source": [
        "tesla_p = tesla_p.drop(2)\n",
        "\n",
        "tesla_d = tesla_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T761LOdtIXL-"
      },
      "source": [
        "Linear Regression\n",
        "This is a class from the sklearn.linear_model module used to create a linear regression model.\n",
        "it attempts to model the relationship between a dependent variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c79MW9g8IK4B"
      },
      "source": [
        "r2_score function from sklearn.metrics is used to calculate the R² (R-squared) score, which is a measure of how well the model's predictions fit the actual data. It provides insights into the proportion of variance in the target variable that is predictable from the independent variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0alAD0G8jDq"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "X = tesla_p[['Sentiment', 'Close_1', 'Close_2', 'Close_3']]  # using sentiment as the feature\n",
        "y = tesla_p['Close']      # predicting the closing price\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
        "\n",
        "# model training\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "r2_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq4tKtVB8oCx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, color='blue', label='Actual vs. Predicted')\n",
        "plt.plot(y_test, y_test, color='red', linestyle='--', label='Ideal Line')\n",
        "plt.xlabel('Actual Values (y_test)')\n",
        "plt.ylabel('Predicted Values (y_pred)')\n",
        "plt.title('Actual vs. Predicted Values: Linear Regression')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnmy2xJyJHKP"
      },
      "source": [
        "StandardScaler -  This is a preprocessing tool from sklearn used to standardize features by removing the mean and scaling to unit variance. Standardization can improve the performance of neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YASAHmMXJQpO"
      },
      "source": [
        "Keras: A high-level neural networks API, Keras is used to build and train deep learning models. It runs on top of TensorFlow or Theano.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngqOGc2xJU-L"
      },
      "source": [
        "Sequential: This class allows you to build a model layer by layer, making it straightforward to define the architecture of a feedforward neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DazKxclDJZqN"
      },
      "source": [
        "Dense: This represents a fully connected layer in the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCjVCPjR8zVb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# standardizing features by removing the mean and scaling to unit variance\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#close1, close2,close3 preivous closing price of 3days\n",
        "X = tesla_p[['Sentiment', 'Close_1', 'Close_2', 'Close_3']]\n",
        "y = tesla_p['Close']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
        "\n",
        "# building the ANN model\n",
        "#artificial neural network  is built using multiple layers,\n",
        "#with each layer consisting of interconnected neurons. The first layer receives input data,\n",
        "# and subsequent layers transform the data through learned weights and activation functions.\n",
        "#ann gather their knowledge by detecing patterns and relationship  in data and learn\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1))  # for regression - no activation function\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "# model training\n",
        "model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suRC63K381eF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, color='blue', label='Actual vs. Predicted')\n",
        "plt.plot(y_test, y_test, color='red', linestyle='--', label='Ideal Line')\n",
        "plt.xlabel('Actual Values (y_test)')\n",
        "plt.ylabel('Predicted Values (y_pred)')\n",
        "plt.title('Actual vs. Predicted Values: ANN')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-G0xnb7xVPy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
